---
title: "Reproducible Research"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


```{r packages, echo=FALSE}
suppressPackageStartupMessages(suppressWarnings({
  library(ipumsr)
  library(tidyverse)
}))

```


# {.tabset}

## Setup - First Time

This script is intended to semi-automate parts of the IPUMS data acquisition process, using the [microdata API]().  In order to run, users must: 
1. Have an ipums account registered with one or more of **USA, CPS**
1. Register for an [IPUMS microdata API key](https://developer.ipums.org/docs/apiprogram/)
1. Add your API key as an [environmental variable](https://tech.popdata.org/ipumsr/dev/)


Some Notes on this file:
This script uses `{.tabset}`s in an attempt to keep the back-end setup and user-facing analysis separate, more organized, and hopefully easier to work with. This script also uses code-folding to keep the knitted HTML report neat.

This script is set up with the intention of being shared via github, but you **should not** include the `.dat.gz` or `.xml` files in the git repo. Consider adding the following, or similar, to `.gitignore`: 
"Data/**.dat.gz",
"Data/**.xml", 
"Data/**.csv"


## Setup - Project Parameters

This template makes as few assumptions as possible, yet automates as many steps as possible. Specify a few project parameters, and this script will check on, submit, and download data straight to your specified directory. Once compiled, only the script itself and the `.json` file are needed to share/reproduce the work in this script. There are 4 **key parameters** the user must define in the code chunk below, with additional details following the parameter chunk.

Once these parameters are specified, Run/Knit this script the first time - It will most likely "fail" and inform you the data is not ready. However, you should now see {r json_filename} in the `data_dir`. (if it wasn't there already). Smaller extracts may be ready in just a few minutes, but extracts with many variables/samples may take longer. Simply re-run/knit this script to check on the extract and, once ready, download straight to your specified directory.

```{r project_paramaters}

#### Key Parameters #####

collection <- "usa" ## c("usa", "cps")

data_dir <- file.path("Data")

descriptive_name <- "prcs_migration_ex"

extract_num <- ""

#### Creating Data Extract 'By Hand' ####

## DO NOT EDIT IF YOU CREATED/SUBMITTED EXTRACT ONLINE ##

extract_definition <- define_extract_micro(
  collection = "",
  description = "",
  samples = c(""),
  variables = c(""),
)


# ## Example values
# extract_definition <- define_extract_micro(
#   collection = "usa",
#   description = "Extract for API vignette",
#   samples = c("us2018a","us2019a"),
#   variables = c("AGE","SEX","RACE","STATEFIP"),
#   data_format = "fixed_width",
#   data_structure = "rectangular",
#   rectangular_on = "P"
# )


```

### Parameter Definitions

**collection** The IPUMS data collection to query, abbreviated and lower-case. Currently only USA and CPS are supported, ie: "usa", "cps"

**data_dir** Where to download your data (will be created if it does not exist). We recommend storing data, dictionaries, and extract definitions within a sub-folder. The default will create a sub-folder named "Data." within your R project. 
* If you want to store your files at the **top-level** of the project directory use: 
  + `data_dir <- file.path("")`.
* If you want to store your files **outside** of the project directory use:
  + `data_dir <- file.path("..","Data")` to store in a sibling-directory to the project directory.
  + The `".."` goes "up" one level within a folder system.

**descriptive_name** IPUMS provides numerical IDs for each data extract by default (eg, `usa_000001.dat.gz, usa_000001.xml`), however these are specific to individual users and can be confusing to keep track of. We recommend users relabel their extracts using a project- or analysis-specific descriptive name, eg: "prcs_migration_ex". The script will automatically apply the same `descriptive_name` to the `.json, .dat.gz, .xml`, as well as a `.csv` file used for checking extract status.

**extract_num** The last step is indicate an extract definition. While the microdata API allows users to build extracts 'by hand' via coding, it also requires users to know the IPUMS mnemonics in order to specify variables, samples, etc. **Most users** will probably still build (and submit) their extract definitions online via the Data Cart GUI on [ipums.org](www.ipums.org). 
* To work with a **past extract**
  + Take note of the **Extract Number** on your [Download or Revise Data page](https://usa.ipums.org/usa-action/data_requests/download).
  + Modify `extract_num` 
  + Run/Knit Script
  
* To work with your **most recent extract**.
  + Leave `extract_num <- ""` as is
  + Run/Knit Script

* To build an extract **by hand**
  + Leave `extract_num <- ""` as is
  + Modify `extract_definition` as needed
  + Run/Knit Script
  
* If you **already have a .json extract definition**
  + Manually add it to the `data_dir` folder
  + Run/Knit Script
  

From here, you can skip ahead to [Analysis Awaits].

#### Initial setup

```{r}

if(!data_dir==""){
if(!dir.exists(data_dir)){
  dir.create(data_dir)
}
}

if(collection==""){
  stop("Please specify a collection, one of c('usa', 'cps')")
}

if(descriptive_name==""){
  stop("Please specify a descriptive name for files")
}

json_filename <- paste0(descriptive_name,".json")
data_rename <- paste0(descriptive_name,".dat.gz")
ddi_rename <- paste0(descriptive_name,".xml")
chk_name <- paste0("chk_",descriptive_name,".csv")

json_present <- file.exists(file.path(data_dir, json_filename))
data_present <- file.exists(file.path(data_dir, data_rename)) &
                   file.exists(file.path(data_dir, ddi_rename))

submitted <- file.exists(file.path(data_dir, chk_name))

if(submitted){
submitted_num <- read.csv(file.path(data_dir, chk_name))[[1]]
}

```


The following handles the various sources of extract definitions, depending on input decisions, creating `extract_info`, which will be used to actually do the checking/downloading.

```{r, eval = !json_present}


if(is.numeric(extract_num)){

  extract_info <- get_extract_info(c(collection, extract_num)) 
  extract_info %>% 
  save_extract_as_json(file = file.path(data_dir,
                                        json_filename)
                       )
} else if ( extract_num==""){

extract_info <- get_last_extract_info(collection)

  extract_info %>%
    save_extract_as_json(file = file.path(data_dir,
                                          json_filename)
                         )
} else {
  extract_info <- extract_definition %>% 
    save_extract_as_json(file = file.path(data_dir,
                                          json_filename)
                         )
}


```

If the json was already present, we skip the above and just read the extract using `ipumsr` functions. 
```{r, eval = json_present}
extract_info <- define_extract_from_json(file.path(data_dir, json_filename), collection)
```

### Submit and Check on Extract

If you built your extract using the ipums.org website you have already submitted your data request. If you built the extract by hand, or your data have expired, the script will resubmit the extract to the IPUMS servers. If this script initiates a submit, it also adds an file to the `data_dir` folder as a flag to prevent submitting multiple copies of the same extract.

```{r, eval = (!data_present) & (!submitted)}

extract_info <- extract_info %>% submit_extract()
write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)

```



Extract number is needed, but user-specific. If a user did not specify an explicit number, this will read it from the submission.

```{r eval = !data_present}

if(submitted){
  extract_info$number <- submitted_num
}

is_data_ready <- is_extract_ready(extract_info)

```

### Download Extract

Now that the JSON is in place, we can check on the data. IPUMS only ensures data will be available for 72 hours, after that point users will need to re-submit an extract request. If your data is out of date, this will re-submit for you.

Once we confirm that the extract is, indeed, ready we download BOTH the data and data dictionary and rename based on the specified values above.

```{r, eval = !data_present}
if(!is_data_ready){
  stop("Extract not ready. Please wait a few mins and re-run file.",call. = F)
} 

ddi_filename <- extract_info %>%
  download_extract(download_dir = data_dir) %>% 
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names 
  file.rename(file.path(data_dir, ddi_filename),
              file.path(data_dir, ddi_rename))
  file.rename(file.path(data_dir, data_filename),
              file.path(data_dir, data_rename))

```

### Load Data

Now we're ready to begin analysis, and your project will be shareable/reproducible for other IPUMS users.

```{r}


ddi <- read_ipums_ddi(file.path(data_dir, ddi_rename))
data <- read_ipums_micro(ddi, data_file = file.path(data_dir, data_rename))

```



## Analysis Awaits {.active}

Begin exploring and visualizng your data!

```{r prep-data}
# Prep education variable
college_regex <- "^[123] year(s)? of college$"
data <- data %>% 
  mutate(
    EDUCD3 = EDUCD %>%
      lbl_collapse(~.val %/% 10) %>% 
      lbl_relabel(
        lbl(2, "Less than High School") ~.val > 0 & .val < 6,
        lbl(3, "High school") ~.lbl == "Grade 12", #<<
        lbl(4, "Some college") ~str_detect(.lbl, college_regex), #<<
        lbl(5, "College or more") ~.val %in% c(10, 11)
      ) %>%
      as_factor()
  )

# Prep income variable
value_to_quintile <- function(x) {
  cut_points <- quantile(x, probs = c(0.2, 0.4, 0.6, 0.8), na.rm = TRUE)
  cut(
    x, 
    breaks = c(-Inf, cut_points, Inf), 
    labels = c("Lowest", "Lower Middle", "Middle", "Upper Middle", "Highest"),
    ordered_result = TRUE
  )
}

hhincome_quintiles <- data %>% 
  filter(PERNUM == 1 & HHINCOME != 9999999) %>% 
  group_by(YEAR) %>% 
  mutate(hhincome_quintile = value_to_quintile(HHINCOME)) %>% 
  ungroup() %>% 
  select(YEAR, SERIAL, hhincome_quintile)

data <- data %>% 
  left_join(hhincome_quintiles, by = c("YEAR", "SERIAL"))

# Prep migration variable
data <- data %>% 
  mutate(
    moved_in_last_year = case_when(
      MIGRATE1 %in% c(0, 9) ~ NA, 
      MIGRATE1 == 1 ~ FALSE, 
      MIGRATE1 %in% 2:4 ~ TRUE
    )
  )


# Prep age variable
age_to_age_group <- function(x) {
  cut_points <- c(9, 17, 34, 64)
  cut(
    x,
    breaks = c(-Inf, cut_points, Inf),
    labels = c("0-9", "10-17", "18-34", "35-64", "65+"),
    ordered_result = TRUE
  )
}

data <- data %>% 
  mutate(age_group = age_to_age_group(AGE))
```

### Migration 2015-2019 {.tabset}

The percentage of people who had moved in the last year increased between 2017 
and 2018 from about 6% to over 8% among all persons in Puerto Rico, but the 
magnitude of this trend varies by education, household income, and age.

Note: These graphs show trends in point estimates from sample data, without 
displaying estimates of sampling error. Differences over time or across groups 
may not be statistically significant. To calculate confidence intervals for 
point estimates, follow the 
[IPUMS USA instructions for using replicate weights](https://usa.ipums.org/usa/repwt.shtml).

#### Overall

```{r migration-graph-1, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT)
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```

#### By education

```{r migration-graph-2, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(AGE >= 25) %>%
  group_by(YEAR, EDUCD3) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~EDUCD3) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      subtitle = "Among persons age 25 and older",
      x = NULL,
      y = "%"
    )
```

#### By household income

```{r migration-graph-3, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(!is.na(hhincome_quintile)) %>% 
  group_by(YEAR, hhincome_quintile) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~hhincome_quintile) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


#### By age

```{r migration-graph-4, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR, age_group) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~age_group) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


