---
title: "Reproducible Research"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.retina = 1,
                      out.width = "75%",
                      out.height = "75%")


```


```{r packages, echo=FALSE}
suppressPackageStartupMessages(suppressWarnings({
  library(ipumsr)
  library(tidyverse)
}))

```

# {.tabset}

## Setup - First Time

This script is intended to semi-automate parts of the IPUMS data acquisition process, using the [microdata API]().  In order to run, users must: 
1. Have an account with one or more IPUMS project
1. Register for an [IPUMS microdata API key](https://developer.ipums.org/docs/apiprogram/)
1. Add your API key as an [environmental variable](https://tech.popdata.org/ipumsr/dev/)


Some Notes on this file:
This script uses `{.tabset}`s and `code-folding` in an attempt to keep the back-end setup and user-facing analysis separate, more organized, and hopefully easier to work with. 

This script is set up with the intention of being shared, either directly or via github. When sharing,  **do not** share the **data** (`.dat.gz`) or **metadata** (`.xml`) files. This script will automatically download those files for the user using the `.json` **extract definition**. If using github, be sure to add the `.dat.gz` and `.xml` files to `.gitignore`, or store them outside the git repo.


## Setup - Project Parameters

This template makes it easy for users to share their custom IPUMS data extracts, without directly sharing microdata. 
* When starting an analysis/project, it handles every step between "creating an extract online" and "analyzing the data". 
* When sharing the template with others, it will automatically submit, download, and compile the report for the end user in just 2 clicks - allowing you to share interactive, data driven reports with colleagues without needing to send massive data files.  

To get started, build your data extract via the Data Cart GUI on [ipums.org](www.ipums.org), and be sure to take note of which IPUMS **collection** you are using (EG, "USA", "CPS"), and which **extract number** you'd like to use. Enter them in the relevant `parameters` below.

Additionally, you'll need to specify a **descriptive name** to help keep track of your files and the **data directory** you'd like your files to go in. With those parameters in place, this script will:

1. Build a `.json` extract definition (if you don't already have one)
1. Submit/Check on extract
1. Download data and metadata to specified directory
1. Relabel files for consistency and legibility.

Fill in the 4 parameters below and click `knit` or run `rmarkdown::render()` to begin. 
* The first time the script will "fail" and inform you the data is not ready. Smaller extracts may be ready in just a few minutes, but extracts with many variables/samples may take longer.
  + If it wasn't already present, you should now see a `.json` file in the `data_dir`.
* Re-running (or re-knitting) the script will check on the status of the specified extract. If it is ready, it will automatically download to the **data_dir** and rename based on **descriptive_name**.

Read on for more info on parameters and the back-end code, or once your data is present, skip ahead to [## Analysis Awaits] to continue as usual. 


```{r project_paramaters}

#### Key Parameters #####

collection <- "usa"

extract_num <- ""

data_dir <- file.path("Data")

descriptive_name <- "prcs_migration_ex"

```

### Parameter Definitions

`collection` The IPUMS data **collection** to query, abbreviated and lower-case. Print `ipums_data_collections()` to see a list of all IPUMS projects and the api-specific name.
  + Note: Currently, only USA and CPS are supported.
  
`extract_num` Which extract from the above collection to use, integer only without leading 0s. Leave blank (`""`) for most recent extract:
  + `extract_num <- 42` For extract "0000042" 
  + `extract_num <- ""` For most recent
  

`data_dir` A **directory** to download your **data** (will be created if it does not exist). We recommend storing data, dictionaries, and extract definitions within a sub-folder. The default will create a sub-folder named "Data" within your R project. 
* If you want to store your files at the **top-level** of the project directory use: 
  + `data_dir <- file.path("")`.
* If you want to store your files **outside** of the project directory use:
  + `data_dir <- file.path("..","Data")` to store in a sibling-directory to the project directory.
  + The `".."` goes "up" one level within a folder system.

`descriptive_name` IPUMS provides numerical IDs for each data extract by default (eg, `usa_000001.dat.gz, usa_000001.xml`), however these are specific to individual users and can be confusing to keep track of. We recommend users relabel their extracts using a project-/analysis- specific **descriptive name**, eg: "prcs_migration_ex". The script will automatically apply the same `descriptive_name` to the `.json, .dat.gz, .xml`, as well as a `.csv` file used for checking extract status.

##### More info on .json

`extract_num` The last step is indicate an extract definition. While the microdata API allows users to build extracts 'by hand' via coding, it also requires users to know the IPUMS mnemonics in order to specify variables/samples. **Most users** will probably still build (and submit) their extract definitions online via the Data Cart GUI on [ipums.org](www.ipums.org). 
* To work with a **past extract**
  + Take note of the **Extract Number** on your [Download or Revise Data page](https://usa.ipums.org/usa-action/data_requests/download).
  + Modify `extract_num` 
  + Run/Knit Script
  
* To work with your **most recent extract**.
  + Leave `extract_num <- ""` as is
  + Run/Knit Script

* To build an extract **by hand**
  + Leave `extract_num <- ""` as is
  + Create `extract_definition` by modifying the arguments of `define_extract_micro()` as needed
  + Run/Knit Script
  
* If you **already have a .json extract definition**
  + Manually add it to the `data_dir` folder
  + Run/Knit Script
  
  
From here, you can skip ahead to [## Analysis Awaits].

#### Initial setup

```{r}


if(!data_dir==""){
if(!dir.exists(data_dir)){
  dir.create(data_dir)
}
}

if(collection==""){
  stop("Please specify a collection, one of c('usa', 'cps')")
}

if(descriptive_name==""){
  stop("Please specify a descriptive name for files")
}


json_filename <- paste0(descriptive_name,".json")
data_rename <- paste0(descriptive_name,".dat.gz")
ddi_rename <- paste0(descriptive_name,".xml")
chk_name <- paste0("chk_",descriptive_name,".csv")

json_present <- file.exists(file.path(data_dir, json_filename))
data_present <- file.exists(file.path(data_dir, data_rename)) &
                   file.exists(file.path(data_dir, ddi_rename))


submitted <- file.exists(file.path(data_dir, chk_name))

if(submitted){
submitted_num <- read.csv(file.path(data_dir, chk_name))[[1]]
}


if(json_present){
  input_json <- list.files(path = data_dir, pattern = ".json")


if(length(input_json) > 1){
  stop("Multiple .json definitions present, please use a separate data_dir for each .json")
}

if(!identical(input_json, json_filename)){
  warning(paste("Updating .json from", input_json, "to", json_filename))
  file.rename(file.path(data_dir, input_json),
              file.path(data_dir, json_filename))
}

}


```


The following handles the various sources of extract definitions, depending on input decisions, creating `extract_info`, which will be used to actually do the checking/downloading.

```{r, eval = !json_present}


if(is.numeric(extract_num)){

  extract_info <- get_extract_info(c(collection, extract_num)) 
  extract_info %>% 
  save_extract_as_json(file = file.path(data_dir,
                                        json_filename)
                       )
  
  write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  stop(".json created, please re-run to check on data")
  
} else if ( extract_num==""){

extract_info <- get_last_extract_info(collection)

  extract_info %>%
    save_extract_as_json(file = file.path(data_dir,
                                          json_filename)
                         )
  
   write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  
  stop(".json created, please re-run to check on data")

}


```

If the json was already present, we skip the above and just read the extract using `ipumsr` functions. 
```{r, eval = json_present}
extract_info <- define_extract_from_json(file.path(data_dir, json_filename))

```

#### Submit, Check on, Download Extract

Now that the JSON is in place, we can check on the data. IPUMS only ensures data will be available for 72 hours, after that point users will need to re-submit an extract request. If your data is out of date, this will re-submit for you.

Once we confirm that the extract is indeed ready we download BOTH the data and data dictionary to `data_dir` based on the `descriptive_name` 

```{r, eval = (!data_present) }

if(submitted){
  extract_info$number <- submitted_num

}
## FIRST check on the extract

if(is.na(extract_info$number)){
  ## fresh read from json, need to submit
  
  extract_info <- extract_info %>% submit_extract()
  save_extract_as_json(extract_info, file.path(data_dir, json_filename))
  write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  
} else {
  ## check on extract
extract_info <- get_extract_info(extract_info)
  already_ready <- is_extract_ready(extract_info)
if ((!already_ready) & extract_info$status=="completed"){
  
  ## stale request, need to re-submit
   extract_info <- extract_info %>% submit_extract()
  save_extract_as_json(extract_info, file.path(data_dir, json_filename))
  write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  stop("Specified extract expired, resubmitting, check back in a few mins")
  
} else if (already_ready & extract_info$status=="incomplete"){
  ## data not ready print warning
   stop("Extract not ready. Please wait a few mins and re-run file.",call. = F)
} else if (already_ready & extract_info$status=="completed"){
  ## get data
  
ddi_filename <- extract_info %>%
  download_extract(download_dir = data_dir) %>% 
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names 
  file.rename(file.path(data_dir, ddi_filename),
              file.path(data_dir, ddi_rename))
  file.rename(file.path(data_dir, data_filename),
              file.path(data_dir, data_rename))
}

}

```


#### Load Data

Now we're ready to begin analysis, and your project will be shareable/reproducible for other IPUMS users.

```{r}
ddi <- read_ipums_ddi(file.path(data_dir, ddi_rename))
data <- read_ipums_micro(ddi, data_file = file.path(data_dir, data_rename))

```


## Analysis Awaits {.active}

```{r prep-data}
# Prep education variable
college_regex <- "^[123] year(s)? of college$"
data <- data %>% 
  mutate(
    EDUCD3 = EDUCD %>%
      lbl_collapse(~.val %/% 10) %>% 
      lbl_relabel(
        lbl(2, "Less than High School") ~.val > 0 & .val < 6,
        lbl(3, "High school") ~.lbl == "Grade 12", #<<
        lbl(4, "Some college") ~str_detect(.lbl, college_regex), #<<
        lbl(5, "College or more") ~.val %in% c(10, 11)
      ) %>%
      as_factor()
  )

# Prep income variable
value_to_quintile <- function(x) {
  cut_points <- quantile(x, probs = c(0.2, 0.4, 0.6, 0.8), na.rm = TRUE)
  cut(
    x, 
    breaks = c(-Inf, cut_points, Inf), 
    labels = c("Lowest", "Lower Middle", "Middle", "Upper Middle", "Highest"),
    ordered_result = TRUE
  )
}

hhincome_quintiles <- data %>% 
  filter(PERNUM == 1 & HHINCOME != 9999999) %>% 
  group_by(YEAR) %>% 
  mutate(hhincome_quintile = value_to_quintile(HHINCOME)) %>% 
  ungroup() %>% 
  select(YEAR, SERIAL, hhincome_quintile)

data <- data %>% 
  left_join(hhincome_quintiles, by = c("YEAR", "SERIAL"))

# Prep migration variable
data <- data %>% 
  mutate(
    moved_in_last_year = case_when(
      MIGRATE1 %in% c(0, 9) ~ NA, 
      MIGRATE1 == 1 ~ FALSE, 
      MIGRATE1 %in% 2:4 ~ TRUE
    )
  )


# Prep age variable
age_to_age_group <- function(x) {
  cut_points <- c(9, 17, 34, 64)
  cut(
    x,
    breaks = c(-Inf, cut_points, Inf),
    labels = c("0-9", "10-17", "18-34", "35-64", "65+"),
    ordered_result = TRUE
  )
}

data <- data %>% 
  mutate(age_group = age_to_age_group(AGE))
```

### Migration 2015-2019 {.tabset}

The percentage of people who had moved in the last year increased between 2017 
and 2018 from about 6% to over 8% among all persons in Puerto Rico, but the 
magnitude of this trend varies by education, household income, and age.

Note: These graphs show trends in point estimates from sample data, without 
displaying estimates of sampling error. Differences over time or across groups 
may not be statistically significant. To calculate confidence intervals for 
point estimates, follow the 
[IPUMS USA instructions for using replicate weights](https://usa.ipums.org/usa/repwt.shtml).

#### Overall

```{r migration-graph-1, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT)
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```

#### By education

```{r migration-graph-2, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(AGE >= 25) %>%
  group_by(YEAR, EDUCD3) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~EDUCD3) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      subtitle = "Among persons age 25 and older",
      x = NULL,
      y = "%"
    )
```

#### By household income

```{r migration-graph-3, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(!is.na(hhincome_quintile)) %>% 
  group_by(YEAR, hhincome_quintile) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~hhincome_quintile) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


#### By age

```{r migration-graph-4, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR, age_group) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~age_group) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


