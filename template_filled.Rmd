---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

```


```{r packages}
suppressPackageStartupMessages(suppressWarnings({
  library(ipumsr)
  library(tidyverse)
}))

```
# Setup
#### First time (ever) setup

This script is intended to semi-automate parts of the IPUMS data acquisition process, using the [microdata API]().  In order to run, users must: 
1. Have an ipums account registered with one or more of **USA, CPS**
1. Register for an [IPUMS microdata API key](https://developer.ipums.org/docs/apiprogram/)
1. Add your API key as an [environmental variable](https://tech.popdata.org/ipumsr/dev/)

#### First time (project) setup

This template makes as few assumptions as possible, yet automates as many steps as possible. 

Fill in the parameters below before proceeding:
You can create multiple Rmd analysis scripts in the same .Rproj directory that each draw on the same IPUMS extract. If you have multiple extracts in the same subdirectory, these parameters will identify the approritate files.

IPUMS provides unique data names to all extracts by default, but we suggest using descriptive names, eg "prcs_migration", etc. And we reccomend using the same descriptive name for the three key files, eg:

1. prcs_migration_ex.json 
1. prcs_migration_ex.dat.gz 
1. prcs_migration_ex.JSON

We recomend storing these three files (and any subsequent extracts relevant to this project) within a sub-folder, `Data`. If you want to store your extract definition, data, and dictionary at the top-project level, you can use `data_dir <- file.path("")`. Or `data_dir <- file.path("..","Data")` if to store your files in a folder at the same level as your `.Rproj`. The `".."` goes "up" one level within a folder system.

Addiotnally, specifiy which IPUMS data collection you will be accessing. With these initial parameters set, we can move on to Creating JSON DEFINITON. If you already have a JSON definition in the folder, the script will proceed to checking and downloading.

**come back to this**

```{r define_proj_names}

data_dir <- file.path("Data")

json_filename <- paste0("prcs_migration_ex",
                        ".json")

data_rename <- paste0("prcs_migration_ex",
                        ".dat.gz")

ddi_rename <- paste0("prcs_migration_ex",
                        ".xml")

collection <- "usa" ## c("usa", "cps")


#### do not edit below ####

json_present <- file.exists(file.path(data_dir, json_filename))
data_present <- file.exists(file.path(data_dir, data_rename)) &
                   file.exists(file.path(data_dir, ddi_rename))
submitted <- file.exists(file.path(data_dir, "chk_submitted.txt"))
if(submitted){
submitted_num <- read.csv(file.path(data_dir, "chk_submitted.txt"))[[1]]
}

```

### CREATE JSON DEFINITION

There are a few options here.

Build totally from scratch. This requires users to know the IPUMS mnemonics in order to specify variables. Probably only for advanced users or simple definitions. **Skip to next**

```{r, eval = !json_present}

# ## Example values
# extract_definition <- define_extract_micro(
#   collection = "usa",
#   description = "Extract for API vignette",
#   samples = c("us2018a","us2019a"),
#   variables = c("AGE","SEX","RACE","STATEFIP"),
#   data_format = "fixed_width",
#   data_structure = "rectangular",
#   rectangular_on = "P"
# )

extract_definition <- define_extract_micro(
  collection = "",
  description = "",
  samples = c(""),
  variables = c(""),
)

```

Or if you have previously defined an extract using the online system, simply take note of the **extract number** and enter it below. If you know your **most recent** extract is the one you'd like to use, simply leave `extract_num <- ""` as is. 

```{r, eval = !json_present}

extract_num <- ""

###################################

if(is.numeric(extract_num)){

  extract_info <- get_extract_info(c(collection, extract_num)) 
  extract_info %>% 
  save_extract_as_json(file = file.path(data_dir,
                                        json_filename)
                       )
} else if ( extract_num==""){

extract_info <- get_last_extract_info(collection)

  extract_info %>%
    save_extract_as_json(file = file.path(data_dir,
                                          json_filename)
                         )
} else {
  extract_info <- extract_definition %>% 
    save_extract_as_json(file = file.path(data_dir,
                                          json_filename)
                         )
}


```

```{r, eval = json_present}
extract_info <- define_extract_from_json(file.path(data_dir, json_filename), collection)
```

### Submit and Check on Extract

If you built your extract using the ipums.org website you submitted your data request already.  your last step to submit the extract and you If you built the extract by hand above, the last bit of code submitted the extract to the IPUMS servers. If this script submits an extract, it adds an empty text file to the data folder as a flag to prevent submitting multiple copies of the same extract.

```{r, eval = (!data_present) & (!submitted)}

extract_info <- extract_info %>% submit_extract()
write.csv(extract_info$number, file.path(data_dir, "chk_submitted.txt"), row.names = F)

```


Small extracts will be ready in seconds to minutes, but large requests can take a while.


```{r eval = !data_present}

if(submitted){
  extract_info$number <- submitted_num
}

is_data_ready <- is_extract_ready(extract_info)

```

### Download Extract

Now that the JSON is in place, we can check on the data. IPUMS only ensures data will be available for 72 hours, after that point users will need to re-submit an extract request. If your data is out of date, this will re-submit for you.

Once we confirm that the extract is, indeed, ready we download BOTH the data and data dictionary and rename based on the specified values above.

```{r, eval = !data_present}
if(!is_data_ready){
  stop("Note: extract not ready. Please wait a few mins and re-run file.",call. = F)
} 

ddi_filename <- extract_info %>%
  download_extract(download_dir = data_dir) %>% 
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names 
  file.rename(file.path(data_dir, ddi_filename),
              file.path(data_dir, ddi_rename))
  file.rename(file.path(data_dir, data_filename),
              file.path(data_dir, data_rename))

```

### Load Data

Now we're ready to begin analysis, and your project will be shareable/reproducible for other IPUMS users.

```{r}


ddi <- read_ipums_ddi(file.path(data_dir, ddi_rename))
data <- read_ipums_micro(ddi, data_file = file.path(data_dir, data_rename))

```

# Analysis Awaits



```{r prep-data}
# Prep education variable
college_regex <- "^[123] year(s)? of college$"
data <- data %>% 
  mutate(
    EDUCD3 = EDUCD %>%
      lbl_collapse(~.val %/% 10) %>% 
      lbl_relabel(
        lbl(2, "Less than High School") ~.val > 0 & .val < 6,
        lbl(3, "High school") ~.lbl == "Grade 12", #<<
        lbl(4, "Some college") ~str_detect(.lbl, college_regex), #<<
        lbl(5, "College or more") ~.val %in% c(10, 11)
      ) %>%
      as_factor()
  )

# Prep income variable
value_to_quintile <- function(x) {
  cut_points <- quantile(x, probs = c(0.2, 0.4, 0.6, 0.8), na.rm = TRUE)
  cut(
    x, 
    breaks = c(-Inf, cut_points, Inf), 
    labels = c("Lowest", "Lower Middle", "Middle", "Upper Middle", "Highest"),
    ordered_result = TRUE
  )
}

hhincome_quintiles <- data %>% 
  filter(PERNUM == 1 & HHINCOME != 9999999) %>% 
  group_by(YEAR) %>% 
  mutate(hhincome_quintile = value_to_quintile(HHINCOME)) %>% 
  ungroup() %>% 
  select(YEAR, SERIAL, hhincome_quintile)

data <- data %>% 
  left_join(hhincome_quintiles, by = c("YEAR", "SERIAL"))

# Prep migration variable
data <- data %>% 
  mutate(
    moved_in_last_year = case_when(
      MIGRATE1 %in% c(0, 9) ~ NA, 
      MIGRATE1 == 1 ~ FALSE, 
      MIGRATE1 %in% 2:4 ~ TRUE
    )
  )


# Prep age variable
age_to_age_group <- function(x) {
  cut_points <- c(9, 17, 34, 64)
  cut(
    x,
    breaks = c(-Inf, cut_points, Inf),
    labels = c("0-9", "10-17", "18-34", "35-64", "65+"),
    ordered_result = TRUE
  )
}

data <- data %>% 
  mutate(age_group = age_to_age_group(AGE))
```

# Migration 2015-2019 {.tabset}

The percentage of people who had moved in the last year increased between 2017 
and 2018 from about 6% to over 8% among all persons in Puerto Rico, but the 
magnitude of this trend varies by education, household income, and age.

Note: These graphs show trends in point estimates from sample data, without 
displaying estimates of sampling error. Differences over time or across groups 
may not be statistically significant. To calculate confidence intervals for 
point estimates, follow the 
[IPUMS USA instructions for using replicate weights](https://usa.ipums.org/usa/repwt.shtml).

## Overall

```{r migration-graph-1, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT)
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```

## By education

```{r migration-graph-2, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(AGE >= 25) %>%
  group_by(YEAR, EDUCD3) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~EDUCD3) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      subtitle = "Among persons age 25 and older",
      x = NULL,
      y = "%"
    )
```

## By household income

```{r migration-graph-3, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(!is.na(hhincome_quintile)) %>% 
  group_by(YEAR, hhincome_quintile) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~hhincome_quintile) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


## By age

```{r migration-graph-4, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR, age_group) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~age_group) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


