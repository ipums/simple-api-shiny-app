---
title: "Reproducible Research"
author: 
data: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
---
```{r project_paramaters}

#### Key Parameters #####

collection <- "usa"

extract_num <- ""

data_dir <- file.path("Data")

descriptive_name <- "template"

```

Welcome to the the Rmd for Reproducible Research template. Fill in the key parameters above and `Knit` to proceed. See TOC or Read on for more information.

# Backend {.tabset} 

## Setup {.tabset}

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


### Load Packages

```{r}

#### Add any additional packages here.
suppressPackageStartupMessages(suppressWarnings({
  library(ipumsr)
  library(tidyverse)
}))
```

### Basic Checks
```{r}

if(!data_dir==""){
if(!dir.exists(data_dir)){
  dir.create(data_dir)
}
}

if(collection==""){
  stop("Please specify a collection, one of c('usa', 'cps')",call. = F)
}

if(descriptive_name==""){
  stop("Please specify a descriptive name for files",call. = F)
}


json_rename <- paste0(descriptive_name,".json")
data_rename <- paste0(descriptive_name,".dat.gz")
ddi_rename <- paste0(descriptive_name,".xml")
chk_name <- paste0("chk_",descriptive_name,".csv")

json_files <- list.files(path = data_dir, pattern = ".json")

json_present <- FALSE

if(length(json_files) > 1){
  stop("Multiple .json definitions present, please use a separate data_dir for each .json",call. = F) 
}


if (length(json_files) == 1) {
  if (!identical(json_files, json_rename)) {
    warning(paste(
      "Updating .json from", json_files,
      "to", json_rename
      )
      )
    file.rename(file.path(data_dir, json_files),
                file.path(data_dir, json_rename))
    
    json_present <- TRUE
  }
  
}

data_present <- file.exists(
  file.path(data_dir, data_rename)) &
  file.exists(file.path(data_dir, ddi_rename)
              )


submitted <- file.exists(file.path(data_dir, chk_name))

if(submitted){
submitted_num <- read.csv(file.path(data_dir, chk_name))[[1]]
}

## eval flags for extract check

stale <- FALSE
waiting <- FALSE
ready <- FALSE

```

## HELP - First Time Setup

This script is intended to automate parts of the IPUMS data acquisition process, using the [microdata API](https://beta.developer.ipums.org/docs/apiprogram/). 
In order to run, users must: 

1. Register for access to IPUMS data, currently two projects have API support:
  * [IPUMS USA](https://usa.ipums.org/usa-action/menu)
  * [IPUMS CPS](https://cps.ipums.org/cps-action/menu)
  * **IPUMS NHGIS coming soon!!**
1. Email `ipums+api@umn.edu` to register for beta access.
1. Generate an [IPUMS microdata API key](https://account.ipums.org/api_keys)
1. Add your API key as an [environment variable](https://tech.popdata.org/ipumsr/reference/set_ipums_api_key.html)




**Some Notes on this file:**

This script is set up with the intention of being shared, either directly or via github. When sharing,  **do not** share the **data** (`.dat.gz`) or **metadata** (`.xml`) files. This script will automatically download those files for the user using the `.json` **extract definition**. If using github, be sure to add the `.dat.gz` and `.xml` files to `.gitignore`, or store them outside the git repo.

This script uses `{.tabset}` and `code-folding` in an attempt to keep the back-end setup and user-facing analysis separate, more organized, and hopefully easier to work with. 



## HELP - Parameter Definitions {.active}

* `collection` The IPUMS data **collection** to query, abbreviated and lower-case. Print `ipums_data_collections()` to see a list of all IPUMS projects and the api-specific name.
  + Note: Currently, only USA and CPS are supported.
  
* `extract_num` Which extract from the above collection to use, integer only without leading 0s. Leave blank (`""`) for most recent extract:
  + `extract_num <- 42` For extract "0000042" 
  + `extract_num <- ""` For most recent
  
* `data_dir` A **directory** to download your **data** (will be created if it does not exist). We recommend storing data, dictionaries, and extract definitions within a sub-folder. The default will create a sub-folder named "Data" within your R project.   + If you want to store your files at the **top-level** of the project directory use: 
    + `data_dir <- file.path("")`.
  + If you want to store your files **outside** of the project directory use:
    + `data_dir <- file.path("..","Data")` to store in a sibling-directory to the project directory.
    + The `".."` goes "up" one level within a folder system.

* `descriptive_name` IPUMS provides numerical IDs for each data extract by default (eg, `usa_000001.dat.gz, usa_000001.xml`), however these are specific to individual users and can be confusing to keep track of. We recommend users relabel their extracts using a project-/analysis- specific **descriptive name**, eg: "prcs_migration_ex". The script will automatically apply the same `descriptive_name` to the `.json, .dat.gz, .xml`, as well as a `.csv` file used for checking extract status.

  

## HELP - The Pipeline
This template makes it easy for users to share their custom IPUMS data extracts, without directly sharing microdata. The script performs 3 main actions: 

* **Step 0:** Create .json
  + Only the original Author of Analysis needs to perform this step.
  + Query an **extract definition** from IPUMS servers
  + Save information as `.json`
  + Check on Extract
    + If ready, download **data** and **metadata**
  + **Note:** This section can be deleted once .json and data are in place
  
* **Step 1:** Check on Extract (AoA or COllab)
  + If extract has not been submitted, submit it (Collab only)
  + If submitted but not ready, prompt wait and rerun (Aoa or Collab)
  + Once ready, download **data** and **metadata**

* **Step 2:** Analysis
  + Author of Analysis writes this section.
  + Collaborator can build on this section.


To get started, build your data extract via the Data Cart GUI on either [IPUMS USA](https://usa.ipums.org/usa-action/variables/group) or [IPUMS CPS](https://cps.ipums.org/cps-action/variables/group) and be sure to take note of which IPUMS **collection** you are using (EG, "usa", "cps"), and which **extract number** you'd like to use (or leave it as `""` for most recent). Enter the relevant `parameters` below, along with a **descriptive name** to help keep track of your files and the **data directory** you'd like your files to go in.

With these 4 parameters set click `knit` or run `rmarkdown::render()` to begin. Read on for more info on parameters and how the back-end code works. Or, once your data is in place, skip ahead to [## Analysis Awaits] to continue as usual. **Some Notes:**

* The first time the script will most likely "fail" and inform the user that a .json has been created. If data are already ready, they will be downloaded
* Once a .json is present, the script will use it to check on a data extract. 
  + If it is not ready, it will cause an "error" and let you know to re-run again.
  + Smaller extracts may be ready in just a few minutes, but extracts with many variables/samples may take longer.
* Once the data files are available from IPUMS servers, the script will automatically download data **and** metadata directly to your  **data_dir** and rename based on **descriptive_name**.
* Default parameters will look for the **most recent USA** extract.
* Use the `CODE` button to the right to show/hide the parameters below, and any other code used in the file.

From here, you can skip ahead to [# Analysis Awaits].

## Step 0: Create .json (can be deleted)

============ Step 0 Author of Analysis only ========================
Everything between the `===` may be deleted once the .json file is in place. If a data extract is already ready, it will be downloaded for the AoA. Even if the data extract is not ready, this section can still safely be deleted, as Step 1 will use the `.json` to check on the extract.


If a .json file is not present, grab the specified extract information from IPUMS servers, either using most recent or an explicit extract number. For either source, it adds a flag file, `chk_descriptive_name.csv`

```{r, eval = !json_present}

if(is.numeric(extract_num)){
  extract_info <- get_extract_info(c(collection, extract_num)) 
} else if ( extract_num==""){
  extract_info <- get_last_extract_info(collection)
}

 extract_info %>% 
  save_extract_as_json(file = file.path(data_dir,
                                        json_rename)
                       )
  
  write.csv(extract_info$number, file.path(data_dir, chk_name), row.names = F)
  
  already_ready <- is_extract_ready(extract_info)
  
  ## stale request, need to re-submit
  stale <- (!already_ready) & extract_info$status == "completed"
  waiting <- already_ready & extract_info$status == "incomplete"
  ready <- already_ready & extract_info$status == "completed"

  if(stale){
    extract_info <- extract_info %>% submit_extract()
  stop(" NOT an error: Specified extract expired, resubmitting, check back in a few mins",call. = F)
  
  } else if (waiting){
    ## data not ready print warning
  stop(" NOT an error:Extract not ready. Please wait a few mins and re-run file.",call. = F)
  
    } else if(ready){
        ## get data
  
  ddi_filename <- extract_info %>%
    download_extract(download_dir = data_dir) %>%
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names
  file.rename(file.path(data_dir, ddi_filename),
              file.path(data_dir, ddi_rename))
  file.rename(file.path(data_dir, data_filename),
              file.path(data_dir, data_rename))
  
  data_present <- TRUE
  
  file.remove(file.path(data_dir, chk_name))

    }
  

```

Everything in the above section can be deleted once the `.json` file is saved in the `data_dir`.

============== END Step 0 (AoA only) ====================================

# Step 1: Get Data

## Submit, Check on, Download Extract

Once the `.json` is present, we begin the process of checking and submitting. `sumitted` is `TRUE` as long as the `chk_.csv` flag file is in the `data_dir`. If this is not present, but a .json is, the script will submit the extract for the first time, in the `else` section below. 


```{r, eval = !data_present}

if(length(json_files)==0){
  stop(".json extract definition expected. Please see step 0 to create .json from online submission.",call. = F)
}

extract_info <- define_extract_from_json(file.path(data_dir, json_rename))


if(submitted) {
  ## read extract number
  extract_info$number <- submitted_num
  ## check on extract
  extract_info <- get_extract_info(extract_info)
  already_ready <- is_extract_ready(extract_info)
  
  ## stale request, need to re-submit
  stale <- (!already_ready) & extract_info$status == "completed"
  waiting <- already_ready & extract_info$status == "incomplete"
  
  ready <- already_ready & extract_info$status == "completed"
  
} else {
  ## submit for the first time
  extract_info <- extract_info %>% submit_extract()
  save_extract_as_json(extract_info, file.path(data_dir, json_rename))
  write.csv(extract_info$number,
            file.path(data_dir, chk_name),
            row.names = F)
  stop(" NOT an error: Extract submitted to IPUMS. Please re-run in a few minutes to check on/download data.", call. = F)
  
}

```


IPUMS only ensures data will be available for 72 hours, after that point users will need to re-submit an extract request. If your data is out of date, this will re-submit for you. If the data are not ready, it will let you know to check back in a few, or if it is ready it will download BOTH the data and data dictionary to `data_dir` based on the `descriptive_name` 

```{r, eval = stale}
  ## stale request, need to re-submit
  
  extract_info <- extract_info %>% submit_extract()
  save_extract_as_json(extract_info, file.path(data_dir, json_rename))
  write.csv(extract_info$number,
            file.path(data_dir, chk_name),
            row.names = F)
  
  stop(" NOT an error: Specified extract expired, resubmitting, check back in a few mins",call. = F)
  
  
```
  
```{r, eval = waiting}  
  ## data not ready print warning
  stop(" NOT an error:Extract not ready. Please wait a few mins and re-run file.",call. = F)
```

```{r, eval = ready}

  ## get data
  
  ddi_filename <- extract_info %>%
    download_extract(download_dir = data_dir) %>%
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names
  file.rename(file.path(data_dir, ddi_filename),
              file.path(data_dir, ddi_rename))
  file.rename(file.path(data_dir, data_filename),
              file.path(data_dir, data_rename))
  
  data_present <- TRUE
  
  file.path(data_dir, chk_name)
  
```


# Step 2: Analysis Awaits {.active}

## Load Data

Now we're ready to begin analysis, and your project will be shareable/reproducible for other IPUMS users.

```{r, eval = data_present}

ddi <- read_ipums_ddi(file.path(data_dir, ddi_rename))
data <- read_ipums_micro(ddi, data_file = file.path(data_dir, data_rename))

```

Example Code:
```{r, eval = data_present}

## Feel free to replace
n_hh <- data %>% distinct(YEAR,SERIAL) %>% nrow()
n_per <- data %>% nrow()
print(paste(paste0("IPUMS ", extract_info$collection, ", extract number ", extract_info$number, "; Description: '", extract_info$description, "'"), paste("This extract contains", n_hh, "household records and", n_per, "person records"),collapse = "\n"))
```



```{r prep-data}
# Prep education variable
college_regex <- "^[123] year(s)? of college$"
data <- data %>% 
  mutate(
    EDUCD3 = EDUCD %>%
      lbl_collapse(~.val %/% 10) %>% 
      lbl_relabel(
        lbl(2, "Less than High School") ~.val > 0 & .val < 6,
        lbl(3, "High school") ~.lbl == "Grade 12", #<<
        lbl(4, "Some college") ~str_detect(.lbl, college_regex), #<<
        lbl(5, "College or more") ~.val %in% c(10, 11)
      ) %>%
      as_factor()
  )

# Prep income variable
value_to_quintile <- function(x) {
  cut_points <- quantile(x, probs = c(0.2, 0.4, 0.6, 0.8), na.rm = TRUE)
  cut(
    x, 
    breaks = c(-Inf, cut_points, Inf), 
    labels = c("Lowest", "Lower Middle", "Middle", "Upper Middle", "Highest"),
    ordered_result = TRUE
  )
}

hhincome_quintiles <- data %>% 
  filter(PERNUM == 1 & HHINCOME != 9999999) %>% 
  group_by(YEAR) %>% 
  mutate(hhincome_quintile = value_to_quintile(HHINCOME)) %>% 
  ungroup() %>% 
  select(YEAR, SERIAL, hhincome_quintile)

data <- data %>% 
  left_join(hhincome_quintiles, by = c("YEAR", "SERIAL"))

# Prep migration variable
data <- data %>% 
  mutate(
    moved_in_last_year = case_when(
      MIGRATE1 %in% c(0, 9) ~ NA, 
      MIGRATE1 == 1 ~ FALSE, 
      MIGRATE1 %in% 2:4 ~ TRUE
    )
  )


# Prep age variable
age_to_age_group <- function(x) {
  cut_points <- c(9, 17, 34, 64)
  cut(
    x,
    breaks = c(-Inf, cut_points, Inf),
    labels = c("0-9", "10-17", "18-34", "35-64", "65+"),
    ordered_result = TRUE
  )
}

data <- data %>% 
  mutate(age_group = age_to_age_group(AGE))
```

# Migration 2015-2019 {.tabset}

The percentage of people who had moved in the last year increased between 2017 
and 2018 from about 6% to over 8% among all persons in Puerto Rico, but the 
magnitude of this trend varies by education, household income, and age.

Note: These graphs show trends in point estimates from sample data, without 
displaying estimates of sampling error. Differences over time or across groups 
may not be statistically significant. To calculate confidence intervals for 
point estimates, follow the 
[IPUMS USA instructions for using replicate weights](https://usa.ipums.org/usa/repwt.shtml).

## Overall

```{r migration-graph-1, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT)
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```

## By education

```{r migration-graph-2, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(AGE >= 25) %>%
  group_by(YEAR, EDUCD3) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~EDUCD3) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      subtitle = "Among persons age 25 and older",
      x = NULL,
      y = "%"
    )
```

## By household income

```{r migration-graph-3, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  filter(!is.na(hhincome_quintile)) %>% 
  group_by(YEAR, hhincome_quintile) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~hhincome_quintile) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


## By age

```{r migration-graph-4, dpi=300, fig.height=5, fig.width=8, echo = FALSE}
data %>% 
  filter(!is.na(moved_in_last_year)) %>% 
  group_by(YEAR, age_group) %>% 
  summarize(
    pct_moved = 100 * sum(PERWT[moved_in_last_year]) / sum(PERWT),
    .groups = "drop"
  ) %>% 
  ggplot(aes(x = YEAR, y = pct_moved)) +
    geom_line() +
    facet_wrap(~age_group) +
    labs(
      title = "Percentage of people who moved in the past year, 2015-2019",
      x = NULL,
      y = "%"
    )
```


## Setup Code

The code below defines the steps of checking for IPUMS data within within the specified `Data` subfolder. If the appropriate `.dat.gz, .xml` files **do not** exist, it triggers code to:
* CREATE a [data extract]() from the included JSON definition. 
* SUBMIT the extract to IPUMS data servers.
* WAIT and periodically check on status of data extract, when ready...
* DOWNLOAD `data` and `data dictionary` to specific directory.
* RENAME files from `usa_000030.xml, usa_000030.dat.gz` to an explicit file name.
* READ IN both `data` and `data dictionary` to current session.

If the file is re-run at a later date, the `.dat.gz` and `.xml` data files will be detected skipping ahead to simply loading these files into R as `data` and `ddi`, respectively. Starting your 
From here, we can 

```{r setup_code, eval = FALSE}


  library(ipumsr)
  library(tidyverse)


## extract definition

## IF the 
json_filename <- "prcs_migration_extract.json"
data_rename <- "prcs_migration_extract.dat.gz"
ddi_rename <- "prcs_migtation_extract.xml" 

collection <- "usa" ## c("usa", "cps")
## maybe include a WAIT_NOW parameter

data_path <- file.path("Data")  ## Specify a sub-directory or filepath

if (!(file.exists(file.path(data_path, data_filename)) & 
      file.exists(file.path(data_path, dict_filename)))) {
  # Load extract definition from JSON
  extract_definition <- define_extract_from_json(
    file.path(data_path, dict_filename),
    collection
  )
  # Submit, wait for, and download extract
  ddi_filename <- submit_extract(extract_definition) %>% 
    wait_for_extract() %>% 
    download_extract(download_dir = data_path) %>% 
    basename()
  # Infer data file name from DDI file name
  data_filename <- str_replace(ddi_filename, "\\.xml$", ".dat.gz")
  # Standardize DDI and data file names 
  file.rename(file.path(data_path, ddi_filename),
              file.path(data_path, "prcs_migration_extract.xml"))
  file.rename(file.path(data_path, data_filename),
              file.path(data_path,"prcs_migration_extract.dat.gz"))
}

ddi <- read_ipums_ddi("Data/prcs_migration_extract.xml")
data <- read_ipums_micro(ddi, data_file = "Data/prcs_migration_extract.dat.gz")

```